{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ntorj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from joblib import dump, load\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>link</th>\n",
       "      <th>research</th>\n",
       "      <th>field</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Prof Sophia Ananiadou</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/s...</td>\n",
       "      <td>\\nResearch interestsProf. Ananiadou's main con...</td>\n",
       "      <td>cs</td>\n",
       "      <td>412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Dr Richard Banach</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/r...</td>\n",
       "      <td>\\nResearch interestsFor accurate and up to dat...</td>\n",
       "      <td>cs</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Prof Gavin Brown</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/g...</td>\n",
       "      <td>\\nResearch interestsMachine Learning.\\n</td>\n",
       "      <td>cs</td>\n",
       "      <td>374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Dr Ke Chen</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/k...</td>\n",
       "      <td>\\nResearch interestsMy general areas of intere...</td>\n",
       "      <td>cs</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Prof Timothy Cootes</td>\n",
       "      <td>https://www.research.manchester.ac.uk/portal/t...</td>\n",
       "      <td>\\nResearch interestsMedical Image Analysis usi...</td>\n",
       "      <td>cs</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   name  \\\n",
       "0           0  Prof Sophia Ananiadou   \n",
       "1           1      Dr Richard Banach   \n",
       "2           2       Prof Gavin Brown   \n",
       "3           3             Dr Ke Chen   \n",
       "4           4    Prof Timothy Cootes   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.research.manchester.ac.uk/portal/s...   \n",
       "1  https://www.research.manchester.ac.uk/portal/r...   \n",
       "2  https://www.research.manchester.ac.uk/portal/g...   \n",
       "3  https://www.research.manchester.ac.uk/portal/k...   \n",
       "4  https://www.research.manchester.ac.uk/portal/t...   \n",
       "\n",
       "                                            research field   id  \n",
       "0  \\nResearch interestsProf. Ananiadou's main con...    cs  412  \n",
       "1  \\nResearch interestsFor accurate and up to dat...    cs  267  \n",
       "2            \\nResearch interestsMachine Learning.\\n    cs  374  \n",
       "3  \\nResearch interestsMy general areas of intere...    cs  170  \n",
       "4  \\nResearch interestsMedical Image Analysis usi...    cs  418  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni = pd.read_csv(\"crawled_uni.csv\")\n",
    "uni.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y =(uni.research, uni.field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ce    297\n",
       "es    189\n",
       "cs     63\n",
       "Name: field, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni['field'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stemmer = WordNetLemmatizer()\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = tfidfconverter.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56  1  3]\n",
      " [ 5  1  2]\n",
      " [ 4  0 38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ce       0.86      0.93      0.90        60\n",
      "          cs       0.50      0.12      0.20         8\n",
      "          es       0.88      0.90      0.89        42\n",
      "\n",
      "    accuracy                           0.86       110\n",
      "   macro avg       0.75      0.65      0.66       110\n",
      "weighted avg       0.84      0.86      0.84       110\n",
      "\n",
      "0.8636363636363636\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classifier.joblib']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(vectorizer, \"preprocessor.joblib\")\n",
    "dump(model, \"classifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = load(\"preprocessor.joblib\")\n",
    "model = load(\"classifier.joblib\")\n",
    "\n",
    "def predict_topic():\n",
    "  text = input(\"Enter text to be predicted: \")\n",
    "  text = preprocessor.transform([text])\n",
    "  prediction = model.predict(text)[0]\n",
    "  print(\"This area is most likely to be in {}.\" .format(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text to be predicted: chemistry\n",
      "This area is most likely to be in es.\n"
     ]
    }
   ],
   "source": [
    "predict_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
